{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNRuQzeQx8/KHmjL/v5gK4d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hyun3246/Warehouse/blob/main/CLIP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import copy\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "2rd0rXe7eFY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Encoder"
      ],
      "metadata": {
        "id": "NTvoIwlOeBYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, d_embed, vocab_size):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_embed)\n",
        "        self.d_embed = d_embed\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.embedding(x) * math.sqrt(self.d_embed)\n",
        "        return out\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_embed, max_len=1024):\n",
        "        super().__init__()\n",
        "        self.pe = nn.Embedding(max_len, d_embed)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, seq_len, _ = x.size()\n",
        "        pos_ids = torch.arange(seq_len, device=x.device)\n",
        "        return x + self.pe(pos_ids)"
      ],
      "metadata": {
        "id": "A1SuzP7WhTPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi-Head Attention Layer: Same as Transformer\n",
        "class MultiHeadAttentionLayer(nn.Module):\n",
        "    def __init__(self, d_model, h, qkv_fc, out_fc):\n",
        "        super(MultiHeadAttentionLayer, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.h = h\n",
        "        self.q_fc = copy.deepcopy(qkv_fc)\n",
        "        self.k_fc = copy.deepcopy(qkv_fc)\n",
        "        self.v_fc = copy.deepcopy(qkv_fc)\n",
        "        self.out_fc = out_fc\n",
        "\n",
        "    def calculate_attention(self, query, key, value, mask):\n",
        "        d_k = key.shape[-1]\n",
        "        attention_score = torch.matmul(query, key.transpose(-2, -1))\n",
        "        attention_score = attention_score / math.sqrt(d_k)\n",
        "\n",
        "        if mask is not None:\n",
        "            attention_score = attention_score.masked_fill(mask==0, -1e9)\n",
        "\n",
        "        attention_prob = F.softmax(attention_score, dim=-1)\n",
        "        out = torch.matmul(attention_prob, value)\n",
        "        return out\n",
        "\n",
        "    def forward(self, *args, query, key, value, mask=None):\n",
        "        n_batch = query.size(0)\n",
        "\n",
        "        def transform(x, fc):\n",
        "            out = fc(x)\n",
        "            out = out.view(n_batch, -1, self.h, self.d_model // self.h)\n",
        "            out = out.transpose(1, 2)\n",
        "            return out\n",
        "\n",
        "        query = transform(query, self.q_fc)\n",
        "        key = transform(key, self.k_fc)\n",
        "        value = transform(value, self.v_fc)\n",
        "\n",
        "        out = self.calculate_attention(query, key, value, mask)\n",
        "        out = out.transpose(1, 2)\n",
        "        out = out.contiguous().view(n_batch, -1, self.d_model)\n",
        "        out = self.out_fc(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class PositionWiseFeedForwardLayer(nn.Module):\n",
        "    def __init__(self, fc1, fc2):\n",
        "        super(PositionWiseFeedForwardLayer, self).__init__()\n",
        "        self.fc1 = fc1\n",
        "        self.relu = nn.GELU()\n",
        "        self.fc2 = fc2\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "class AddNormLayer(nn.Module):\n",
        "    def __init__(self, d_model, dropout_ratio=0.1):\n",
        "        super(AddNormLayer, self).__init__()\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "    def forward(self, x, sub_layer):\n",
        "        out = self.norm(x)\n",
        "        out = sub_layer(out)\n",
        "        out = self.dropout(out)\n",
        "        out = out + x\n",
        "        return out"
      ],
      "metadata": {
        "id": "yq1q462qfgrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVfkhweQcqnT"
      },
      "outputs": [],
      "source": [
        "class TextEncoderBlock(nn.Module):\n",
        "    def __init__(self, self_attention, position_ff, d_model, dropout_ratio=0.1):\n",
        "        super(TextEncoderBlock, self).__init__()\n",
        "        self.self_attention = self_attention\n",
        "        self.position_ff = position_ff\n",
        "        self.residuals = nn.ModuleList([AddNormLayer(d_model, dropout_ratio) for _ in range(2)])\n",
        "\n",
        "    def forward(self, src, src_mask=None):\n",
        "        out = src\n",
        "        out = self.residuals[0](out, lambda out: self.self_attention(query=out, key=out, value=out, mask=src_mask))\n",
        "        out = self.residuals[1](out, self.position_ff)\n",
        "        return out\n",
        "\n",
        "# TextEncoder: Text encoder for CLIP\n",
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, token_embed, pos_embed, encoder_block, n_layer, d_model, d_out, dropout_ratio=0.1):\n",
        "        super(TextEncoder, self).__init__()\n",
        "\n",
        "        # embedding layers\n",
        "        self.token_embed = token_embed\n",
        "        self.pos_embed = pos_embed\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "        # encoder blocks\n",
        "        self.layers = nn.ModuleList([copy.deepcopy(encoder_block) for _ in range(n_layer)])\n",
        "\n",
        "        # final layer norm and text projection\n",
        "        self.ln_final = nn.LayerNorm(d_model)\n",
        "        self.text_projection = nn.Parameter(torch.empty(d_model, d_out))\n",
        "        nn.init.normal_(self.text_projection, std=d_model ** -0.5)\n",
        "\n",
        "    # build_causal_mask(): CLIP uses causal mask, which masks the future tokens.\n",
        "    def build_causal_mask(self, seq_len, device):\n",
        "        mask = torch.triu(torch.ones(seq_len, seq_len, device=device), diagonal=1)\n",
        "        return mask == 0\n",
        "\n",
        "    def forward(self, text_tokens):\n",
        "        seq_len = text_tokens.shape[1]\n",
        "        mask = self.build_causal_mask(seq_len, text_tokens.device)\n",
        "\n",
        "        # 1. Embedding layers\n",
        "        out = self.token_embed(text_tokens)\n",
        "        out = self.pos_embed(out)\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        # 2. Encoder blocks\n",
        "        for layer in self.layers:\n",
        "            out = layer(out, mask)\n",
        "\n",
        "        # 3. Use EOS token\n",
        "        eos_indices = text_tokens.argmax(dim=-1)\n",
        "        out = out[torch.arange(out.shape[0]),eos_indices]\n",
        "\n",
        "        # 4. Layer norm and text projection\n",
        "        out = self.ln_final(out)\n",
        "        out = out @ self.text_projection\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Image Encoder"
      ],
      "metadata": {
        "id": "4L8XEEgpxGR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Patch Embedding\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, in_channels=3, patch_size=4, d_embed=512):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "        # Instead of manually dividing images, use Conv layer.\n",
        "        self.projection = nn.Conv2d(in_channels, d_embed, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, 3, 32, 32)\n",
        "        out = self.projection(x)    # (batch, d_embed, 8, 8)\n",
        "        out = out.flatten(2)        # (batch, d_embed, 64)\n",
        "        out = out.transpose(1, 2)   # (batch, 64, d_embed)\n",
        "        return out\n",
        "\n",
        "# Use the same positonal encoding of text encoder"
      ],
      "metadata": {
        "id": "qhXsENwtxKgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the same classes of text encoder.\n",
        "# PositionWiseFeedForwardLayer\n",
        "# AddNormLayer\n",
        "# MultiHeadAttentionLayer\n",
        "# EncoderBlock\n",
        "\n",
        "class CLIPVisionEncoder(nn.Module):\n",
        "    def __init__(self, patch_embed, pos_embed, layers, d_model, d_out):\n",
        "        super().__init__()\n",
        "        self.patch_embed = patch_embed\n",
        "        self.pos_embed = pos_embed\n",
        "        self.layers = layers\n",
        "\n",
        "        self.ln_pre = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, d_model))\n",
        "\n",
        "        self.ln_post = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.vision_projection = nn.Parameter(torch.empty(d_model, d_out))\n",
        "        nn.init.normal_(self.vision_projection, std=d_model ** -0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1. Patch embedding\n",
        "        out = self.patch_embed(x)\n",
        "\n",
        "        # 2. Add CLS token\n",
        "        batch_size = out.shape[0]\n",
        "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
        "        out = torch.cat((cls_tokens, out), dim=1)\n",
        "\n",
        "        # 3. Add positional embedding\n",
        "        out = self.pos_embed(out)\n",
        "\n",
        "        # 4. Pre layer norm\n",
        "        out = self.ln_pre(out)\n",
        "\n",
        "        # 5. Image encoder\n",
        "        for layer in self.layers:\n",
        "            out = layer(out)\n",
        "\n",
        "        # 6. Use only CLS token\n",
        "        out = out[:, 0]\n",
        "\n",
        "        # 7. Post layer norm and vision projection\n",
        "        out = self.ln_post(out)\n",
        "        out = out @ self.vision_projection\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "MPhR_MoVyX7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CLIP"
      ],
      "metadata": {
        "id": "wrElAkZx1cPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CLIP(nn.Module):\n",
        "    def __init__(self, image_encoder, text_encoder):\n",
        "        super().__init__()\n",
        "        self.visual = image_encoder\n",
        "        self.transformer = text_encoder\n",
        "\n",
        "        # learnable temperature parameter\n",
        "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
        "\n",
        "    def forward(self, image, text):\n",
        "        # 1. Get image and text features from each of encoder.\n",
        "        image_features = self.visual(image)\n",
        "        text_features = self.transformer(text)\n",
        "\n",
        "        # 2. L2 normalization\n",
        "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        # 3. Cosine similarity and scaling\n",
        "        logit_scale = self.logit_scale.exp()        # multiply temperature\n",
        "        logits_per_image = logit_scale * image_features @ text_features.t()     # Cosine similarity and scaling\n",
        "        logits_per_text = logits_per_image.t()\n",
        "\n",
        "        return logits_per_image, logits_per_text\n",
        "\n",
        "# clip_loss(): Compute symmetric cross entropy\n",
        "def clip_loss(logits_per_image, logits_per_text):\n",
        "    device = logits_per_image.device\n",
        "\n",
        "    batch_size = logits_per_image.shape[0]\n",
        "\n",
        "    # 1. Create answer labels (Indices of diagonal)\n",
        "    labels = torch.arange(batch_size, device=device)\n",
        "\n",
        "    # 2. Compute loss\n",
        "    loss_i = F.cross_entropy(logits_per_image, labels)      # Image -> text\n",
        "    loss_t = F.cross_entropy(logits_per_text, labels)       # Text -> image\n",
        "\n",
        "    return (loss_i + loss_t) / 2"
      ],
      "metadata": {
        "id": "boqJmOnl1dHY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
