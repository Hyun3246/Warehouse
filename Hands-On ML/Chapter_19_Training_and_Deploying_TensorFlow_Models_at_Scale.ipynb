{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Tensorflow Model Serving"
      ],
      "metadata": {
        "id": "pao9KF9nTHUk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using TF Serving"
      ],
      "metadata": {
        "id": "qZ9HSjPp4lFc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZhWMFpDSpzd"
      },
      "outputs": [],
      "source": [
        "# make and save model\n",
        "from pathlib import Path\n",
        "import tensorflow as tf\n",
        "\n",
        "mnist = tf.keras.datasets.mnist.load_data()\n",
        "(X_train_full, y_train_full), (X_test, y_test) = mnist\n",
        "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
        "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "tf.keras.backend.clear_session()\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=[28, 28], dtype=tf.uint8),\n",
        "    tf.keras.layers.Rescaling(scale=1 / 255),\n",
        "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=tf.keras.optimizers.SGD(learning_rate=1e-2),\n",
        "              metrics=[\"accuracy\"])\n",
        "model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))\n",
        "\n",
        "model_name = \"my_mnist_model\"\n",
        "model_version = \"0001\"\n",
        "model_path = Path(model_name) / model_version\n",
        "model.save(model_path, save_format=\"tf\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# inspect SavedModel.\n",
        "# Output will be a 'tag', which is a classification of metagraph(calculation graph + function signature(e.g. type, input & output size)).\n",
        "!saved_model_cli show --dir '{model_path}'"
      ],
      "metadata": {
        "id": "JFBFtU1OTdPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at the 'tag set' above.\n",
        "# Output will be a two signature definition, '__saved_model_init_op' and 'serving_default'.\n",
        "!saved_model_cli show --dir '{model_path}' --tag_set serve"
      ],
      "metadata": {
        "id": "HpjP2UPhUMa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Look closely at the basic serving function 'serving_default'.\n",
        "!saved_model_cli show --dir '{model_path}' --tag_set serve \\\n",
        "                      --signature_def serving_default"
      ],
      "metadata": {
        "id": "1NyTaR3_Uv9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install tensorflow serving\n",
        "url = \"https://storage.googleapis.com/tensorflow-serving-apt\"\n",
        "src = \"stable tensorflow-model-server tensorflow-model-server-universal\"\n",
        "!echo 'deb {url} {src}' > /etc/apt/sources.list.d/tensorflow-serving.list\n",
        "!curl '{url}/tensorflow-serving.release.pub.gpg' | apt-key add -\n",
        "!apt update -q && apt-get install -y tensorflow-model-server\n",
        "%pip install -q -U tensorflow-serving-api==2.11.1"
      ],
      "metadata": {
        "id": "qsI9kh7BU46Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"MODEL_DIR\"] = str(model_path.parent.absolute())"
      ],
      "metadata": {
        "id": "5_Ddp_GVWQcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# implement server\n",
        "%%bash --bg\n",
        "tensorflow_model_server \\\n",
        "    --port=8500 \\\n",
        "    --rest_api_port=8501 \\\n",
        "    --model_name=my_mnist_model \\\n",
        "    --model_base_path=\"${MODEL_DIR}\" >my_server.log 2>&1"
      ],
      "metadata": {
        "id": "U_mH9QsSW5zX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# query to TF serving using REST API\n",
        "# make a request\n",
        "import json\n",
        "\n",
        "X_new = X_test[:3]\n",
        "request_json = json.dumps({\n",
        "    \"signature_name\": \"serving_default\",\n",
        "    \"instances\": X_new.tolist()\n",
        "})"
      ],
      "metadata": {
        "id": "3a2SAFq5XjEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# json is 100% text\n",
        "request_json"
      ],
      "metadata": {
        "id": "97gBW5IfX8D4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# deliver request data to TF serving using HTTP POST method\n",
        "import requests\n",
        "\n",
        "server_url = \"http://localhost:8501/v1/models/my_mnist_model:predict\"\n",
        "response = requests.post(server_url, data=request_json)\n",
        "response.raise_for_status()\n",
        "response = response.json()"
      ],
      "metadata": {
        "id": "8NqJeWzAX9Sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make a prediction\n",
        "import numpy as np\n",
        "y_proba = np.array(response['predictions'])\n",
        "y_proba.round(2)"
      ],
      "metadata": {
        "id": "J_L6tf0LYeDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# query to TF serving using gRPC API\n",
        "# Make a request.\n",
        "# Make a PredictRequest protocol buffer and fill in fields.\n",
        "from tensorflow_serving.apis.predict_pb2 import PredictRequest\n",
        "\n",
        "request = PredictRequest()\n",
        "request.model_spec.name = model_name\n",
        "request.model_sepc.signature_name = 'serving_default'\n",
        "input_name = model.input_names[0]\n",
        "request.inputs[input_name].CopyFrom(tf.make_tensor_proto(X_new))"
      ],
      "metadata": {
        "id": "CkmPUz_uZp8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import grpc\n",
        "from tensorflow_serving.apis import prediction_service_pb2_grpc\n",
        "\n",
        "# make a channel\n",
        "channel = grpc.insecure_channel('localhost:8500')\n",
        "\n",
        "# make a gRPC service for the channel\n",
        "predict_service = prediction_service_pb2_grpc.PredictServiceStub(channel)\n",
        "\n",
        "# send a request\n",
        "response = predict_service.Predict(request, timeout=10.0)"
      ],
      "metadata": {
        "id": "OVwmqswBaP3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# change protocol buffer to tensor\n",
        "output_name = model.output_names[0]\n",
        "outputs_proto = response.outputs[output_name]\n",
        "y_proba = tf.make_ndarray(outputs_proto)"
      ],
      "metadata": {
        "id": "-vJdzWZQa_9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make a new version of model\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=[28, 28], dtype=tf.uint8),\n",
        "    tf.keras.layers.Rescaling(scale=1 / 255),\n",
        "    tf.keras.layers.Dense(50, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(50, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=tf.keras.optimizers.SGD(learning_rate=1e-2),\n",
        "              metrics=[\"accuracy\"])\n",
        "history = model.fit(X_train, y_train, epochs=10,\n",
        "                    validation_data=(X_valid, y_valid))"
      ],
      "metadata": {
        "id": "iodYp7UUtAC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save a new version of model\n",
        "model_version = \"0002\"\n",
        "model_path = Path(model_name) / model_version\n",
        "model.save(model_path, save_format=\"tf\")"
      ],
      "metadata": {
        "id": "Wf2Mk4YztFxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vertex AI"
      ],
      "metadata": {
        "id": "YlESlu5Zb5xf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Authorization\n",
        "from google.colab import auth\n",
        "\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "OFqGrQr2b-Jm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make GCS bucket to save SavedModel.\n",
        "from google.cloud import storage\n",
        "\n",
        "project_id = 'my_project'\n",
        "bucket_name = 'my_bucket'\n",
        "location = 'us-central1'\n",
        "\n",
        "storage_client = storage.Client(project=project_id)\n",
        "bucket = storage_client.create_bucket(bucket_name, location=location)"
      ],
      "metadata": {
        "id": "zptnZiEMcnsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a function to upload directory to a new bucket.\n",
        "def upload_directory(bucket, dirpath):\n",
        "    dirpath = Path(dirpath)\n",
        "    for filepath in dirpath.glob(\"**/*\"):\n",
        "        if filepath.is_file():\n",
        "            blob = bucket.blob(filepath.relative_to(dirpath.parent).as_posix())\n",
        "            blob.upload_from_filename(filepath)\n",
        "    upload_directory(bucket, \"my_mnist_model\")"
      ],
      "metadata": {
        "id": "d47Yc9fidR0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# multithreading\n",
        "!gsutil -m cp -r my_mnist_model gs://{bucket_name}/"
      ],
      "metadata": {
        "id": "cLFlHDind1L3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inform Vertex AI about the model.\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "server_image = 'gcr.io/cloud-aiplatform/prediction/tf2-gpu.2-8:latest'\n",
        "\n",
        "aiplatform.init(project=project_id, location=location)\n",
        "mnist_model = aiplatform.Model.upload(\n",
        "    display_name='mnist',\n",
        "    artifact_uri=f'gs://{bucket_name}/my_mnist_model/0001',\n",
        "    serving_container_image_uri=server_image,\n",
        ")"
      ],
      "metadata": {
        "id": "mpdBf1r9eOxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make endpoint\n",
        "endpoint = aiplatform.Endpoint.create(display_name='mnist-endpoint')\n",
        "\n",
        "endpoint.deploy(\n",
        "    mnist_model,\n",
        "    min_replica_count=1,\n",
        "    max_replica_count-5,\n",
        "    machine_type='n1-standard-4',\n",
        "    accelerator_type='NVIDIA_TESLA_K80',\n",
        "    accelerator_count=1\n",
        ")"
      ],
      "metadata": {
        "id": "J99DY23ie3e5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make a prediction\n",
        "response = endpoint.predict(instances=X_new.tolist())"
      ],
      "metadata": {
        "id": "Vdzfw2Tbfi-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.round(response.predictions, 2)"
      ],
      "metadata": {
        "id": "JqhggxgRfphm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove endpoint\n",
        "endpoint.undeploy_all()\n",
        "endpoint.delete()"
      ],
      "metadata": {
        "id": "rhZG2hMffv42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Batch prediction on Vertex AI"
      ],
      "metadata": {
        "id": "8bZOc1cDgJ-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare batch and upload to GCS\n",
        "# make JSON Lines file\n",
        "batch_path = Path('my_mnist_batch')\n",
        "batch_path.mkdir(exist_ok=True)\n",
        "with open(batch_path / 'my_mnist_batch.jsonl', 'w') as jsonl_file:\n",
        "    for image in X_test[:100].tolist():\n",
        "        jsonl_file.write(json.dumps(image))\n",
        "        jsonl.file.write('\\n')\n",
        "\n",
        "upload_directory(bucket, batch_path)"
      ],
      "metadata": {
        "id": "htowIdRHgTFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set directory path\n",
        "batch_prediction_job = mnist_model.batch_predict(\n",
        "    job_display_name=\"my_batch_prediction_job\",\n",
        "    machine_type=\"n1-standard-4\",\n",
        "    starting_replica_count=1,\n",
        "    max_replica_count=5,\n",
        "    accelerator_type=\"NVIDIA_TESLA_K80\",\n",
        "    accelerator_count=1,\n",
        "    gcs_source=[f\"gs://{bucket_name}/{batch_path.name}/my_mnist_batch.jsonl\"],\n",
        "    gcs_destination_prefix=f\"gs://{bucket_name}/my_mnist_predictions/\",\n",
        "    sync=True\n",
        ")"
      ],
      "metadata": {
        "id": "qkPlxenDhFB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make predictions\n",
        "y_probas = []\n",
        "for blob in batch_prediction_job.iter_outputs():\n",
        "    if 'prediction.results' in blob.name:\n",
        "        for line in blob.download_as_text().splitlines():\n",
        "            y_proba = json.loads(line)['prediction']\n",
        "            y_probas.append(y_proba)"
      ],
      "metadata": {
        "id": "6vvyCKkMhhdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# accuracy\n",
        "y_pred = np.argmax(y_probas, axis=1)\n",
        "accuracy = np.sum(y_pred == y_test[:100]) / 100"
      ],
      "metadata": {
        "id": "uY175b5biFn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# delete model, bucket and batch prediction job\n",
        "for prefix in ['my_mnist_model/', 'my_mnist_batch/', 'my_mnist_predictions/']:\n",
        "    blobs = bucket.list_blobs(prefix=prefix)\n",
        "    for blob in blobs:\n",
        "        blob.delete()\n",
        "\n",
        "bucket.delete()\n",
        "batch_prediction_job.delete()"
      ],
      "metadata": {
        "id": "GUmuvbmbiZ2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Distribute models on mobile or embeded device"
      ],
      "metadata": {
        "id": "ib6yG9zqkVWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert SavedModel to FlatBuffers and save as .tflite\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(str(model_path))\n",
        "tflite_model = converter.convert()\n",
        "with open(\"my_converted_savedmodel.tflite\", 'wb') as f:\n",
        "    f.write(tflite_model)"
      ],
      "metadata": {
        "id": "-tZmcz1fkbj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# after-training quantization\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]"
      ],
      "metadata": {
        "id": "yj-SOliblv-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use GPU to Speed Up"
      ],
      "metadata": {
        "id": "J5VoYCYdow2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check tensorflow recognizes GPU\n",
        "physical_gpus = tf.config.list_physical_devices('GPU')\n",
        "physical_gpus"
      ],
      "metadata": {
        "id": "i9FTIXjWo0CD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set RAM limit of tensorflow\n",
        "for gpu in physical_gpus:\n",
        "    tf.config.set_logical_device_configuration(\n",
        "        gpu,\n",
        "        [tf.config.LogicalDeviceConfiguration(memory_limit=2048)]\n",
        "    )"
      ],
      "metadata": {
        "id": "fRGXPcF4pk8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make tensorflow occupy GPU only if necessary\n",
        "for gpu in physical_gpus:\n",
        "    tf.config.experimental.set_memory_growth(gpu, True)"
      ],
      "metadata": {
        "id": "AoZYg1Jtp7hY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# divide GPU into 2+ logical devices\n",
        "tf.config.set_logical_device_configuration(\n",
        "    physical_gpus[0],\n",
        "    [tf.config.LogicalDeviceConfiguration(memory_limit=2048),\n",
        "     tf.config.LogicalDeviceConfiguration(memory_limit=2048)]\n",
        ")"
      ],
      "metadata": {
        "id": "uGp7o4FaqL4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check logical devices\n",
        "logical_gpus = tf.config.list_logical_devices('GPU')\n",
        "logical_gpus"
      ],
      "metadata": {
        "id": "u2hhspgQqdo9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Allocating Computation and Variable to Device"
      ],
      "metadata": {
        "id": "OsJah8beq8SZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = tf.Variable([1., 2., 3.])\n",
        "a.device    # check device\n",
        "b = tf.Variable([1, 2, 3])\n",
        "b.device"
      ],
      "metadata": {
        "id": "HfQGVA-jrGXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# change device\n",
        "with tf.device(\"/cpu:0\"):\n",
        "    c = tf.Variable([1., 2., 3.])\n",
        "c.device"
      ],
      "metadata": {
        "id": "HAkb-NZYreI7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}